import gradio as gr
import time
from pathlib import Path
import torchaudio
from stepaudio import StepAudio
import os
from fastapi import FastAPI, HTTPException, Depends, Query, Body
from fastapi.security import APIKeyHeader
from fastapi.openapi.docs import get_swagger_ui_html, get_redoc_html
from fastapi.openapi.utils import get_openapi
from typing import Optional, Dict, Any, List
from pydantic import BaseModel
from status_tracker import StatusTracker

from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

# æ·»åŠ Tokenè®¤è¯é…ç½®
DEFAULT_TOKEN = "step_audio_778899"
AUTH_TOKEN = os.environ.get("STEP_AUDIO_TOKEN", DEFAULT_TOKEN)

# ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é»˜è®¤å€¼è®¾ç½®ç›®å½•
MODELS_DIR = os.environ.get("MODELS_DIR", "./models")
CONFIG_DIR = os.environ.get("CONFIG_DIR", "./config")
DATA_DIR = os.environ.get("DATA_DIR", "./data")
LOGS_DIR = os.environ.get("LOGS_DIR", "./logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(os.path.join(DATA_DIR, "output"), exist_ok=True)
os.makedirs(os.path.join(DATA_DIR, "cache"), exist_ok=True)

# è°ƒæ•´ä¸´æ—¶ç¼“å­˜ç›®å½•
CACHE_DIR = os.path.join(DATA_DIR, "cache")

# åˆ›å»ºçŠ¶æ€è·Ÿè¸ªå™¨
status_tracker = StatusTracker()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Step-Audio API",
    description="Step-Audio è¯­éŸ³åˆæˆå’Œè¯­éŸ³èŠå¤©æœåŠ¡API",
    version="1.0.0",
    docs_url=None,  # ç¦ç”¨é»˜è®¤çš„æ–‡æ¡£URL
    redoc_url=None, # ç¦ç”¨é»˜è®¤çš„ReDoc URL
)

# APIå®‰å…¨ç›¸å…³
api_key_header = APIKeyHeader(name="X-API-KEY", auto_error=False)

# Pydanticæ¨¡å‹ç”¨äºAPIè¯·æ±‚å’Œå“åº”
class HealthResponse(BaseModel):
    status: str
    version: str
    uptime: int

class GPUInfo(BaseModel):
    id: int
    name: str
    memory_total: float
    memory_used: float
    memory_free: float
    utilization: float

class GPUResourceResponse(BaseModel):
    gpu_count: int
    gpus: List[GPUInfo]

class ModelInfo(BaseModel):
    loaded: bool
    path: Optional[str] = None
    load_time: Optional[str] = None

class ModelStatusResponse(BaseModel):
    models: Dict[str, ModelInfo]
    ready: bool

class CPUInfo(BaseModel):
    percent: float
    cores: int

class MemoryInfo(BaseModel):
    total: float
    available: float
    percent: float

class DiskInfo(BaseModel):
    total: float
    free: float
    percent: float

class SystemResourceResponse(BaseModel):
    cpu: CPUInfo
    memory: MemoryInfo
    disk: DiskInfo

class MessageRequest(BaseModel):
    text: str
    token: str

class MessageResponse(BaseModel):
    response: str
    audio_path: str


# ä¾èµ–å‡½æ•°ï¼Œç”¨äºAPIè®¤è¯
async def verify_token(api_key: str = Depends(api_key_header)):
    if api_key == AUTH_TOKEN:
        return True
    raise HTTPException(
        status_code=401,
        detail="æ— æ•ˆçš„APIå¯†é’¥",
        headers={"WWW-Authenticate": "Bearer"},
    )


class CustomAsr:
    def __init__(self, model_name="iic/SenseVoiceSmall", device="cuda"):
        self.model = AutoModel(
            model=model_name,
            vad_model="fsmn-vad",
            vad_kwargs={"max_single_segment_time": 30000},
            device=device,
        )

    def run(self, audio_path):
        res = self.model.generate(
            input=audio_path,
            cache={},
            language="auto",  # "zh", "en", "yue", "ja", "ko", "nospeech"
            use_itn=True,
            batch_size_s=60,
            merge_vad=True,  #
            merge_length_s=15,
        )
        text = rich_transcription_postprocess(res[0]["text"])
        return text


def add_message(chatbot, history, mic, text):
    if not mic and not text:
        return chatbot, history, "Input is empty"

    if text:
        chatbot.append({"role": "user", "content": text})
        history.append({"role": "user", "content": text})
    elif mic and Path(mic).exists():
        chatbot.append({"role": "user", "content": {"path": mic}})
        history.append({"role": "user", "content": {"type":"audio", "audio": mic}})

    print(f"{history=}")
    return chatbot, history, None


def reset_state(system_prompt):
    """Reset the chat history."""
    return [], [{"role": "system", "content": system_prompt}]


def save_tmp_audio(audio, sr):
    import tempfile

    with tempfile.NamedTemporaryFile(
        dir=CACHE_DIR, delete=False, suffix=".wav"
    ) as temp_audio:
        temp_audio_path = temp_audio.name
        torchaudio.save(temp_audio_path, audio, sr)

    return temp_audio.name


def predict(chatbot, history, audio_model, asr_model):
    """Generate a response from the model."""
    try:
        is_input_audio = False
        user_audio_path = None
        # æ£€æµ‹ç”¨æˆ·è¾“å…¥çš„æ˜¯éŸ³é¢‘è¿˜æ˜¯æ–‡æœ¬
        if isinstance(history[-1]["content"], dict):
            is_input_audio = True
            user_audio_path = history[-1]["content"]["audio"]
        text, audio, sr = audio_model(history, "Tingting")
        print(f"predict {text=}")
        audio_path = save_tmp_audio(audio, sr)
        # ç¼“å­˜ç”¨æˆ·è¯­éŸ³çš„ asr æ–‡æœ¬ç»“æœä¸ºäº†åŠ é€Ÿä¸‹ä¸€æ¬¡æ¨ç†
        if is_input_audio:
            asr_text = asr_model.run(user_audio_path)
            chatbot.append({"role": "user", "content": asr_text})
            history[-1]["content"] = asr_text
            print(f"{asr_text=}")
        chatbot.append({"role": "assistant", "content": {"path": audio_path}})
        chatbot.append({"role": "assistant", "content": text})
        history.append({"role": "assistant", "content": text})
    except Exception as e:
        print(e)
        gr.Warning(f"Some error happend, retry submit")
    return chatbot, history


# Tokenè®¤è¯éªŒè¯å‡½æ•°
def validate_token(token):
    """éªŒè¯ç”¨æˆ·è¾“å…¥çš„tokenæ˜¯å¦æœ‰æ•ˆ"""
    if token == AUTH_TOKEN:
        return True
    return False


# è‡ªå®šä¹‰æ–‡æ¡£è·¯ç”±
@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui_html():
    """è‡ªå®šä¹‰Swagger UIé¡µé¢è·¯ç”±"""
    return get_swagger_ui_html(
        openapi_url="/openapi.json",
        title=app.title + " - Swagger UI",
        oauth2_redirect_url=app.swagger_ui_oauth2_redirect_url,
        swagger_js_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui-bundle.js",
        swagger_css_url="https://cdn.jsdelivr.net/npm/swagger-ui-dist@5/swagger-ui.css",
    )

@app.get("/redoc", include_in_schema=False)
async def redoc_html():
    """è‡ªå®šä¹‰ReDocé¡µé¢è·¯ç”±"""
    return get_redoc_html(
        openapi_url="/openapi.json",
        title=app.title + " - ReDoc",
        redoc_js_url="https://cdn.jsdelivr.net/npm/redoc@next/bundles/redoc.standalone.js",
    )

@app.get("/openapi.json", include_in_schema=False)
async def get_open_api_endpoint():
    """OpenAPI schemaç«¯ç‚¹"""
    return get_openapi(
        title=app.title,
        version=app.version,
        description=app.description,
        routes=app.routes,
    )


# APIè·¯ç”±
@app.get("/api/health", response_model=HealthResponse, tags=["çŠ¶æ€"])
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£ï¼Œè¿”å›æœåŠ¡çŠ¶æ€ä¿¡æ¯"""
    return status_tracker.get_health_info()

@app.get("/api/model/status", response_model=ModelStatusResponse, tags=["çŠ¶æ€"])
async def model_status(auth: bool = Depends(verify_token)):
    """æ¨¡å‹çŠ¶æ€æ£€æŸ¥æ¥å£ï¼Œè¿”å›æ¨¡å‹åŠ è½½çŠ¶æ€"""
    return status_tracker.get_model_status()

@app.get("/api/resources/gpu", response_model=GPUResourceResponse, tags=["èµ„æº"])
async def gpu_resources(auth: bool = Depends(verify_token)):
    """GPUèµ„æºæ£€æŸ¥æ¥å£ï¼Œè¿”å›GPUä½¿ç”¨æƒ…å†µ"""
    return status_tracker.get_gpu_info()

@app.get("/api/resources/system", response_model=SystemResourceResponse, tags=["èµ„æº"])
async def system_resources(auth: bool = Depends(verify_token)):
    """ç³»ç»Ÿèµ„æºæ£€æŸ¥æ¥å£ï¼Œè¿”å›CPUã€å†…å­˜å’Œç£ç›˜ä½¿ç”¨æƒ…å†µ"""
    return status_tracker.get_system_info()

@app.post("/api/chat/message", response_model=MessageResponse, tags=["èŠå¤©"])
async def chat_message(
    request: MessageRequest = Body(..., description="èŠå¤©è¯·æ±‚"),
    auth: bool = Depends(verify_token)
):
    """å‘é€èŠå¤©æ¶ˆæ¯å¹¶è·å–å›å¤ï¼Œæ”¯æŒæ–‡æœ¬è¾“å…¥ï¼Œè¿”å›æ–‡æœ¬å’Œè¯­éŸ³å›å¤"""
    global audio_model, asr_model
    
    if not validate_token(request.token):
        raise HTTPException(
            status_code=401,
            detail="è®¤è¯å¤±è´¥ï¼Œè¯·æä¾›æœ‰æ•ˆçš„è®¿é—®ä»¤ç‰Œ"
        )
    
    # æ„å»ºå†å²è®°å½•
    history = [
        {"role": "system", "content": "é€‚é…ç”¨æˆ·çš„è¯­è¨€ï¼Œç”¨ç®€çŸ­å£è¯­åŒ–çš„æ–‡å­—å›ç­”"},
        {"role": "user", "content": request.text}
    ]
    
    # ç”Ÿæˆå›å¤
    text, audio, sr = audio_model(history, "Tingting")
    audio_path = save_tmp_audio(audio, sr)
    
    return {
        "response": text,
        "audio_path": audio_path
    }


def _launch_demo(args, audio_model, asr_model):
    with gr.Blocks(delete_cache=(86400, 86400)) as demo:
        gr.Markdown("""<center><font size=8>Step Audio Chat</center>""")
        
        # æ·»åŠ Tokenè®¤è¯è¾“å…¥æ¡†
        auth_token = gr.Textbox(
            label="Authentication Token",
            type="password",
            placeholder="è¯·è¾“å…¥è®¿é—®ä»¤ç‰Œ",
        )
        
        with gr.Row():
            system_prompt = gr.Textbox(
                label="System Prompt",
                value="é€‚é…ç”¨æˆ·çš„è¯­è¨€ï¼Œç”¨ç®€çŸ­å£è¯­åŒ–çš„æ–‡å­—å›ç­”",
                lines=2
            )
            
        chatbot = gr.Chatbot(
            elem_id="chatbot",
            avatar_images=["assets/user.png", "assets/assistant.png"],
            min_height=800,
            type="messages",
        )
        # ä¿å­˜ chat å†å²ï¼Œä¸éœ€è¦æ¯æ¬¡å†é‡æ–°æ‹¼æ ¼å¼
        history = gr.State([{"role": "system", "content": system_prompt.value}])
        mic = gr.Audio(type="filepath")
        text = gr.Textbox(placeholder="Enter message ...")

        with gr.Row():
            clean_btn = gr.Button("ğŸ§¹ Clear History (æ¸…é™¤å†å²)")
            regen_btn = gr.Button("ğŸ¤”ï¸ Regenerate (é‡è¯•)")
            submit_btn = gr.Button("ğŸš€ Submit")

        def on_submit(chatbot, history, mic, text, token):
            # å…ˆéªŒè¯token
            if not validate_token(token):
                gr.Warning("è®¤è¯å¤±è´¥ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„è®¿é—®ä»¤ç‰Œ")
                return chatbot, history, mic, text
                
            chatbot, history, error = add_message(
                chatbot, history, mic, text
            )
            if error:
                gr.Warning(error)  # æ˜¾ç¤ºè­¦å‘Šæ¶ˆæ¯
                return chatbot, history, None, None
            else:
                chatbot, history = predict(chatbot, history, audio_model, asr_model)
                return chatbot, history, None, None

        submit_btn.click(
            fn=on_submit,
            inputs=[chatbot, history, mic, text, auth_token],
            outputs=[chatbot, history, mic, text],
            concurrency_limit=4,
            concurrency_id="gpu_queue",
        )
        
        def reset_with_auth(system_prompt, token):
            # éªŒè¯token
            if not validate_token(token):
                gr.Warning("è®¤è¯å¤±è´¥ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„è®¿é—®ä»¤ç‰Œ")
                return chatbot, history
            return reset_state(system_prompt)
            
        clean_btn.click(
            fn=reset_with_auth,
            inputs=[system_prompt, auth_token],
            outputs=[chatbot, history],
            show_progress=True,
        )

        def regenerate_with_auth(chatbot, history, token):
            # éªŒè¯token
            if not validate_token(token):
                gr.Warning("è®¤è¯å¤±è´¥ï¼Œè¯·è¾“å…¥æœ‰æ•ˆçš„è®¿é—®ä»¤ç‰Œ")
                return chatbot, history
                
            while chatbot and chatbot[-1]["role"] == "assistant":
                chatbot.pop()
            while history and history[-1]["role"] == "assistant":
                print(f"discard {history[-1]}")
                history.pop()
            return predict(chatbot, history, audio_model, asr_model)

        regen_btn.click(
            regenerate_with_auth,
            [chatbot, history, auth_token],
            [chatbot, history],
            show_progress=True,
            concurrency_id="gpu_queue",
        )

    # å°†Gradioåº”ç”¨æŒ‚è½½åˆ°FastAPIåº”ç”¨
    gr_app = gr.mount_gradio_app(app, demo, path="/")
    return app


if __name__ == "__main__":
    from argparse import ArgumentParser
    import os
    import uvicorn

    parser = ArgumentParser()
    parser.add_argument("--model-path", type=str, required=True, help="Model path.")
    parser.add_argument(
        "--server-port", type=int, default=7860, help="Demo server port."
    )
    parser.add_argument(
        "--server-name", type=str, default="0.0.0.0", help="Demo server name."
    )
    parser.add_argument(
        "--share", action="store_true", help="Enable sharing of the demo."
    )
    parser.add_argument(
        "--gpu-ids", type=str, default="", help="GPU IDs to use, comma separated (e.g. '0,1')"
    )
    parser.add_argument(
        "--device-map", type=str, default="auto", 
        help="Model loading device mapping strategy (auto, balanced, sequential)"
    )
    args = parser.parse_args()

    # è§£æGPUé…ç½®
    gpu_ids = [int(x) for x in args.gpu_ids.split(",")] if args.gpu_ids else None
    
    # è®¾ç½®æ¨¡å‹è·¯å¾„
    tokenizer_path = os.path.join(args.model_path, "Step-Audio-Tokenizer")
    tts_path = os.path.join(args.model_path, "Step-Audio-TTS-3B")
    llm_path = os.path.join(args.model_path, "Step-Audio-Chat")
    
    # åˆå§‹åŒ–æ¨¡å‹
    audio_model = StepAudio(
        tokenizer_path=tokenizer_path,
        tts_path=tts_path,
        llm_path=llm_path,
        device_map=args.device_map,
        gpu_ids=gpu_ids
    )
    
    # æ›´æ–°æ¨¡å‹çŠ¶æ€
    status_tracker.update_model_status("tokenizer", True, tokenizer_path)
    status_tracker.update_model_status("tts", True, tts_path)
    status_tracker.update_model_status("llm", True, llm_path)
    
    asr_model = CustomAsr()
    app = _launch_demo(args, audio_model, asr_model)
    
    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        app, 
        host=args.server_name, 
        port=args.server_port
    )